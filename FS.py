import requests
import urllib.parse
import xml.etree.ElementTree as ET
import re
import csv
import sys
import time

# --- ì„¤ì • ---
API_KEY = "e9T9pUGmWkfF7HJW8BZH%2BFiHHi9AQo1pFvc55gAO"
BASE_URL = "https://franchise.ftc.go.kr/api/search.do"
OUTPUT_FILE = "franchise_full_list.csv"  # íŒŒì¼ëª… ë³€ê²½ (ì „ì²´ ë¦¬ìŠ¤íŠ¸)

# ì„œë²„ ì—°ê²° ì„¤ì •
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
}

def decode_key(key):
    return urllib.parse.unquote(key)

def clean_text(text):
    if not text: return ""
    text = text.replace('<br>', ' ').replace('</p>', ' ').replace('</tr>', ' ').replace('</td>', ' ')
    text = re.sub(r'<[^>]+>', ' ', text)
    return re.sub(r'\s+', ' ', text).strip()

# ğŸ”¥ [í•µì‹¬ ë¡œì§] ì—…ì¢… í…ìŠ¤íŠ¸ ì •ì œ (ëŒ€ë¶„ë¥˜ ì¶”ì¶œ)
def refine_category(text):
    if not text: return "ì‹ë³„ì‹¤íŒ¨"
    
    # 1. ë„ì–´ì“°ê¸°ê°€ í¬í•¨ëœ ëŒ€ë¶„ë¥˜ ì˜ˆì™¸ ì²˜ë¦¬ (ë¶™ì—¬ì“°ê¸°ë¡œ ë³€í™˜)
    text = text.replace("ê¸°íƒ€ ë„ì†Œë§¤", "ê¸°íƒ€ë„ì†Œë§¤")
    text = text.replace("ê¸°íƒ€ ì„œë¹„ìŠ¤", "ê¸°íƒ€ì„œë¹„ìŠ¤")
    text = text.replace("ê¸°íƒ€ ì™¸ì‹", "ê¸°íƒ€ì™¸ì‹")
    text = text.replace("ê¸°íƒ€ êµìœ¡", "ê¸°íƒ€êµìœ¡")
    text = text.replace("ë„ì†Œë§¤ (ìœ í†µ)", "ë„ì†Œë§¤(ìœ í†µ)")
    
    # 2. ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ìª¼ê°œì„œ ê°€ì¥ ì• ë‹¨ì–´(ëŒ€ë¶„ë¥˜)ë§Œ ê°€ì ¸ì˜´
    return text.split()[0]

def extract_industry_final(full_text):
    if not full_text: return "ë‚´ìš©ì—†ìŒ"
    
    # ì±•í„° ë²”ìœ„ í•œì •
    chapter_match = re.search(r"3\.\s*\[.*?\]\s*ì—…ì¢…(.*?)(?=\s4\.|4\.\s|ë°”ë¡œ ì „)", full_text)
    target_text = full_text
    if chapter_match:
        target_text = chapter_match.group(1)

    # íŒ¨í„´ ë§¤ì¹­ ë° ê²°ê³¼ ì •ì œ
    
    # Pattern A
    pattern_a = r"ì†Œë¶„ë¥˜\s*\(.*?ì£¼ìš”ìƒí’ˆ.*?\)\s*(.*?)(?=\s\d+\.|ë‹¨ìœ„|4\.|ì§€ì—­|$)"
    match = re.search(pattern_a, target_text)
    if match: 
        return refine_category(match.group(1).strip())

    # Pattern B
    pattern_b = r"ê°€ë§¹ì‚¬ì—…ì˜ ì¢…ë¥˜\s*[:;]?\s*(.*?)(?=\s\d+\.|ë‹¨ìœ„|4\.|$)"
    match = re.search(pattern_b, target_text)
    if match: 
        return refine_category(match.group(1).strip())

    # Pattern C
    pattern_c = r"ì—…ì¢…\s+([ê°€-í£]+)\s+(ëŒ€ë¶„ë¥˜|ì†Œë¶„ë¥˜)"
    match = re.search(pattern_c, target_text)
    if match: 
        return refine_category(match.group(1).strip())

    return "ì‹ë³„ì‹¤íŒ¨"

def run_full_crawler():
    service_key = decode_key(API_KEY)
    
    print(f"--- ğŸš€ ì „ì²´ ìˆ˜ì§‘ ì‹œì‘: {OUTPUT_FILE}ì— ì €ì¥í•©ë‹ˆë‹¤ ---")
    print("--- âš ï¸  ë°ì´í„° ì–‘ì´ ë§ì•„ ì‹œê°„ì´ ì†Œìš”ë©ë‹ˆë‹¤. ì¤‘ë‹¨í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”. ---")
    
    # íŒŒì¼ ì—´ê¸° (ì“°ê¸° ëª¨ë“œ)
    with open(OUTPUT_FILE, 'w', encoding='utf-8-sig', newline='') as f:
        writer = csv.writer(f)
        writer.writerow(['ì¼ë ¨ë²ˆí˜¸', 'ë¸Œëœë“œëª…', 'ìƒí˜¸(ë²•ì¸ëª…)', 'ì¶”ì¶œëœ_ì—…ì¢…'])

        page_no = 1
        total_collected = 0
        
        while True: # ë¬´í•œ ë£¨í”„ (ë°ì´í„° ëë‚  ë•Œê¹Œì§€)
            params = {
                'type': 'list', 
                'yr': '2023', 
                'serviceKey': service_key,
                'viewType': 'xml', 
                'pageNo': str(page_no), 
                'numOfRows': '50' # í•œ í˜ì´ì§€ë‹¹ 50ê°œì”© ìš”ì²­ (ì†ë„ í–¥ìƒ)
            }
            
            try:
                # ë¦¬ìŠ¤íŠ¸ ìš”ì²­
                res = requests.get(BASE_URL, params=params, headers=HEADERS)
                if res.status_code != 200:
                    print(f"\n[í˜ì´ì§€ {page_no}] ìš”ì²­ ì‹¤íŒ¨. ìƒíƒœ ì½”ë“œ: {res.status_code}")
                    break

                root = ET.fromstring(res.text)
                items = root.findall('.//item')
                
                # ì¢…ë£Œ ì¡°ê±´: í•´ë‹¹ í˜ì´ì§€ì— ì•„ì´í…œì´ ì—†ìœ¼ë©´ ë
                if not items:
                    print(f"\n\nğŸ [ìˆ˜ì§‘ ì™„ë£Œ] ë” ì´ìƒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ì´ {total_collected}ê°œ)")
                    break
                
                # ì•„ì´í…œ ìˆœíšŒ
                for item in items:
                    sn = item.findtext('jngIfrmpSn')
                    brand = item.findtext('brandNm')
                    corp = item.findtext('corpNm')
                    
                    # ìƒì„¸ ë‚´ìš© ìš”ì²­ (content)
                    c_params = {'type': 'content', 'jngIfrmpSn': sn, 'serviceKey': service_key, 'viewType': 'xml'}
                    
                    industry = "ì¡°íšŒì‹¤íŒ¨" # ê¸°ë³¸ê°’
                    try:
                        c_res = requests.get(BASE_URL, params=c_params, headers=HEADERS, timeout=10)
                        if c_res.status_code == 200:
                            # ì—…ì¢… ì¶”ì¶œ ë° ì •ì œ ì‹¤í–‰
                            industry = extract_industry_final(clean_text(c_res.text))
                    except Exception:
                        industry = "íƒ€ì„ì•„ì›ƒ"

                    # CSV ì“°ê¸°
                    writer.writerow([sn, brand, corp, industry])
                    total_collected += 1
                    
                    # ì§„í–‰ ìƒí™© ì¶œë ¥
                    sys.stdout.write(f"\r[P.{page_no}] ëˆ„ì  {total_collected}ê°œ | {brand[:8]:<10} -> {industry}")
                    sys.stdout.flush()
                
                # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                page_no += 1
                # time.sleep(0.5) # í˜ì´ì§€ ë„˜ê¹€ ê°„ê²© (ì„œë²„ ë³´í˜¸ìš©)

            except Exception as e:
                print(f"\n[ì¹˜ëª…ì  ì˜¤ë¥˜ ë°œìƒ] {e}")
                print("ì§„í–‰ ë‚´ìš©ì„ ì €ì¥í•˜ê³  ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break

if __name__ == "__main__":
    run_full_crawler()